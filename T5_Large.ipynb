{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook focuse on the Training the T5-large model and saving it to the Google bucket as suggested novasearch and Training T5 model in the orginal github repository. As done in the novasearch github we train and test on the canard dataset"
      ],
      "metadata": {
        "id": "HHrSBMY-4eGZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1oYShhVXcuw",
        "outputId": "59cc0495-4819-46ef-9ede-1cde5663d3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "\u001b[K     |████████████████████████████████| 153 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 116 kB 50.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 385 kB 44.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 306 kB 53.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 32.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 41.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 48.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 32.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 8.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 44.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 5.0 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 17.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 438 kB 16.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 22.7 MB/s \n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 516.4 MB 15 kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 40.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 454 kB 61.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 561 kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 36.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 55.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 951 kB 43.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 947 kB 37.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 947 kB 51.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 939 kB 48.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 937 kB 38.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 935 kB 49.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 944 kB 47.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 936 kB 45.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 946 kB 56.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 924 kB 41.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 924 kB 41.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 926 kB 31.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 912 kB 56.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 896 kB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 887 kB 53.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 873 kB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 850 kB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 72.0 MB 109 kB/s \n",
            "\u001b[K     |████████████████████████████████| 71.3 MB 39 kB/s \n",
            "\u001b[K     |████████████████████████████████| 69.0 MB 87.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68.5 MB 125 kB/s \n",
            "\u001b[K     |████████████████████████████████| 62.5 MB 51.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68.5 MB 122 kB/s \n",
            "\u001b[K     |████████████████████████████████| 65.4 MB 66 kB/s \n",
            "\u001b[K     |████████████████████████████████| 65.1 MB 84 kB/s \n",
            "\u001b[K     |████████████████████████████████| 62.2 MB 118 kB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gcs-config==2.1.3\n",
            "  Downloading tensorflow_gcs_config-2.1.3-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 3.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: tensorflow-gcs-config\n",
            "  Attempting uninstall: tensorflow-gcs-config\n",
            "    Found existing installation: tensorflow-gcs-config 2.8.0\n",
            "    Uninstalling tensorflow-gcs-config-2.8.0:\n",
            "      Successfully uninstalled tensorflow-gcs-config-2.8.0\n",
            "Successfully installed tensorflow-gcs-config-2.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow_io-gcs-config==0.23.1 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow_io-gcs-config==0.23.1\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.3\n",
            "  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 45 kB/s \n",
            "\u001b[?25hRequirement already satisfied: t5 in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Collecting tensorflow-text==2.3\n",
            "  Downloading tensorflow_text-2.3.0-cp37-cp37m-manylinux1_x86_64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 38.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.37.1)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.1.2)\n",
            "Collecting tensorboard<3,>=2.3.0\n",
            "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 33.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.14.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.15.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.47.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.6.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.2.0)\n",
            "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /usr/local/lib/python3.7/dist-packages (from t5) (0.1.21)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from t5) (1.12.0+cu113)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from t5) (4.21.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5) (1.0.2)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (from t5) (0.1.2)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from t5) (2.10.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from t5) (3.7)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from t5) (0.5.3)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from t5) (2.2.0)\n",
            "Requirement already satisfied: seqio in /usr/local/lib/python3.7/dist-packages (from t5) (0.0.9)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from t5) (0.1.97)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from t5) (0.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from t5) (1.3.5)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.7/dist-packages (from t5) (4.6.0.dev202208110044)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5) (4.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=2.7.0->t5) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->t5) (2022.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->t5) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->t5) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->t5) (2.8.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu->t5) (4.9.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu->t5) (2.5.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->t5) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu->t5) (0.4.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5) (3.1.0)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from seqio->t5) (0.2.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from seqio->t5) (0.1.75)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib->seqio->t5) (1.12)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (2.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (0.10.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (0.3.5.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (5.9.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (1.9.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (0.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (1.56.4)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, scipy, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.2.3\n",
            "    Uninstalling tensorflow-2.2.3:\n",
            "      Successfully uninstalled tensorflow-2.2.3\n",
            "  Attempting uninstall: tensorflow-text\n",
            "    Found existing installation: tensorflow-text 2.2.0\n",
            "    Uninstalling tensorflow-text-2.2.0:\n",
            "      Successfully uninstalled tensorflow-text-2.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.4.1 tensorboard-2.10.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 tensorflow-text-2.3.0\n",
            "Setting up GCS access...\n",
            "Running on TPU: grpc://10.126.120.2:8470\n",
            "WARNING: auth.authenticate_user() will eventually stop supporting auth for Tensorflow on TPU devices. See auth.authenticate_service_account() instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "#Installing the dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5\n",
        "!pip install -q t5 tensorflow-text==2.2.\n",
        "!pip install -U tensorflow-gcs-config==2.1.3\n",
        "!pip install tensorflow_io-gcs-config==0.23.1\n",
        "!pip install tensorflow==2.3 t5 tensorflow-text==2.3\n",
        "#%env USE_AUTH_EPHEM=0\n",
        "import functools\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "import t5.models\n",
        "import seqio\n",
        "\n",
        "# Required to fix Colab flag parsing issue.\n",
        "sys.argv = sys.argv[:1]\n",
        "\n",
        "BASE_DIR = \"gs://trecuofg\" #@param { type: \"string\" }\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data/canard_data\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  # Use legacy GCS authentication method.\n",
        "  os.environ['USE_AUTH_EPHEM'] = '0'\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v2-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.enable_eager_execution()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCVbezBSXvih"
      },
      "outputs": [],
      "source": [
        "# Getting the paths from Google bucket\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "\n",
        "canard_tsv_path = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"training_t5_canard_data.tsv\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"validation_t5_canard_data.tsv\"),\n",
        "    \"test\": os.path.join(DATA_DIR, \"test_t5_canard_data.tsv\"),\n",
        "    \"cast_test\": os.path.join(DATA_DIR, \"trec_cast_evaluation_predicted_t5.tsv\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JVk1SDDebqL",
        "outputId": "771a779d-6dde-48e9-a724-e196cc3772fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few raw validation examples...\n",
            "{'context': b'What group disbanded? [CTX] Frank Zappa [TURN] Disbandment', 'rewrite': b'What group disbanded?'}\n",
            "{'context': b'When did they disband? [CTX] Frank Zappa [TURN] Disbandment [TURN] What group disbanded? [TURN] Zappa and the Mothers of Invention', 'rewrite': b'When did Zappa and the Mothers of Invention disband?'}\n",
            "{'context': b'What kind of music did they play? [CTX] Frank Zappa [TURN] Disbandment [TURN] What group disbanded? [TURN] Zappa and the Mothers of Invention [TURN] When did they disband? [TURN] In late 1969, Zappa broke up the band.', 'rewrite': b'What kind of music did Zappa and the Mothers of Invention play?'}\n",
            "{'context': b'Why did they break up? [CTX] Frank Zappa [TURN] Disbandment [TURN] What group disbanded? [TURN] Zappa and the Mothers of Invention [TURN] When did they disband? [TURN] In late 1969, Zappa broke up the band. [TURN] What kind of music did they play? [TURN] major influence on the development of the jazz-rock fusion genre.', 'rewrite': b'Why did Zappa and the Mothers of Invention break up?'}\n",
            "{'context': b\"Why were there financial problems? [CTX] Frank Zappa [TURN] Disbandment [TURN] What group disbanded? [TURN] Zappa and the Mothers of Invention [TURN] When did they disband? [TURN] In late 1969, Zappa broke up the band. [TURN] What kind of music did they play? [TURN] major influence on the development of the jazz-rock fusion genre. [TURN] Why did they break up? [TURN] He often cited the financial strain as the main reason, but also commented on the band members' lack of sufficient effort.\", 'rewrite': b'Why were there financial problems with Zappa and the Mothers of Invention?'}\n"
          ]
        }
      ],
      "source": [
        "#Splitting the dataset \n",
        "def canard_dataset_fn(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(canard_tsv_path[split])\n",
        "  # Split each \"<context>\\t<rewrite>\" example into (context, rewrite) tuple.\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  # Map each tuple to a {\"context\": ... \"rewrite\": ...} dict.\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"context\", \"rewrite\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(canard_dataset_fn(\"validation\").take(5)):\n",
        "  print(ex)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSVwbKEt1AiQ"
      },
      "outputs": [],
      "source": [
        "# Preprocessing Canard Dataset\n",
        "def canard_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
        "    #text = tf.strings.lower(text)\n",
        "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"context\": ..., \"rewrite\": ...}->{\"inputs\":w ..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"canard context: \", normalize_text(ex[\"context\"])]),\n",
        "        \"targets\": normalize_text(ex[\"rewrite\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a task registery all mix\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"all_mix\",\n",
        "    dataset_fn=canard_dataset_fn,\n",
        "    splits=[\"train\", \"validation\", \"test\", \"cast_test\"],\n",
        "    text_preprocessor=[canard_preprocessor],\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, t5.evaluation.metrics.bleu, t5.evaluation.metrics.rouge], \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrkPS0X2z4ef",
        "outputId": "0e1beeb4-3b61-4fb8-f031-05dc72bf72e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f541d9e55d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4LcWPdLLO-8"
      },
      "outputs": [],
      "source": [
        "#Setting up the model\n",
        "MODEL_SIZE = \"large\" #@param[\"small\", \"base\", \"large\", \"3B\", \"11B\"]\n",
        "FOLDER_TO_STORE = \"_temperature_0\" #@param { type: \"string\" }\n",
        "\n",
        "# Public GCS path for T5 pre-trained model checkpoints\n",
        "BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n",
        "PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n",
        "MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE + FOLDER_TO_STORE)\n",
        "\n",
        "if ON_CLOUD and MODEL_SIZE == \"3B\":\n",
        "  tf.logging.warning(\n",
        "      \"The `3B` model is too large to use with the 5GB GCS free tier. \"\n",
        "      \"Make sure you have at least 25GB on GCS before continuing.\"\n",
        "  )\n",
        "elif ON_CLOUD and MODEL_SIZE == \"11B\":\n",
        "  raise ValueError(\n",
        "      \"The `11B` parameter is too large to fine-tune on the `v2-8` TPU \"\n",
        "      \"provided by Colab. Please comment out this Error if you're running \"\n",
        "      \"on a larger TPU.\"\n",
        "  )\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU \n",
        "\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from this paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 256, \"targets\": 64},\n",
        "    learning_rate_schedule=0.003,\n",
        "    save_checkpoints_steps=5000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finetuning the steps by setting it at 5000\n",
        "FINETUNE_STEPS = 5000 #@param {type: \"integer\"}\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"all_mix\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixHAXdja15o2",
        "outputId": "efe68ed5-5777-4632-9d12-ce7e94e26b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5-data/pretrained_models/large/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5-data/pretrained_models/large/operative_config.gin\n",
            "INFO:root:Skipping import of unknown module `t5.data.sentencepiece_vocabulary` (skip_unknown=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instal necessary dependancy\n",
        "!pip install numpy==1.19.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "FqN77WyJFMSI",
        "outputId": "b06962bd-c33b-4d58-d4bd-8f80601be97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.19.3\n",
            "  Using cached numpy-1.19.3-cp37-cp37m-manylinux2010_x86_64.whl (14.9 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.3 which is incompatible.\n",
            "tensorflow 2.3.0 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.19.3 which is incompatible.\n",
            "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.4.1 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.3 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a larger batch size for evaluation, which requires less memory.\n",
        "model.batch_size = train_batch_size \n",
        "model.eval(\n",
        "    mixture_or_task_name=\"all_mix\",\n",
        "    checkpoint_steps=\"all\",\n",
        "    split=\"validation\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqyrt1sN1--k",
        "outputId": "d6050d84-408b-449e-9d68-512a2427cc60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://trecuofg/models/large_temperature_0/operative_config.gin\n",
            "ERROR:root:Path not found: gs://trecuofg/models/large_temperature_0/operative_config.gin\n",
            "INFO:absl:Adding task 'all_mix' with predict metric_fn(s).\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Skipping packing/padding for 'all_mix' since sequence length is None.\n",
            "INFO:absl:Setting sequence lengths to {'inputs': 607, 'targets': 75}\n",
            "INFO:absl:Evaluating checkpoint step: 1000700\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 607, 'targets': 75}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:834: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1000700: 0.000\n",
            "INFO:absl:eval/all_mix/bleu at step 1000700: 2.299\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 14.14, 95% confidence [13.71, 14.62]\n",
            "INFO:absl:rouge2 = 7.10, 95% confidence [6.77, 7.42]\n",
            "INFO:absl:rougeLsum = 12.18, 95% confidence [11.79, 12.57]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1000700: 14.135\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1000700: 7.100\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1000700: 12.175\n",
            "INFO:absl:Evaluating checkpoint step: 1000700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 607, 'targets': 75}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1000700: 0.000\n",
            "INFO:absl:eval/all_mix/bleu at step 1000700: 2.299\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 14.16, 95% confidence [13.73, 14.58]\n",
            "INFO:absl:rouge2 = 7.09, 95% confidence [6.78, 7.41]\n",
            "INFO:absl:rougeLsum = 12.18, 95% confidence [11.77, 12.56]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1000700: 14.159\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1000700: 7.094\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1000700: 12.184\n",
            "INFO:absl:Evaluating checkpoint step: 1000700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 607, 'targets': 75}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1000700: 0.000\n",
            "INFO:absl:eval/all_mix/bleu at step 1000700: 2.299\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 14.17, 95% confidence [13.72, 14.64]\n",
            "INFO:absl:rouge2 = 7.11, 95% confidence [6.79, 7.45]\n",
            "INFO:absl:rougeLsum = 12.18, 95% confidence [11.78, 12.56]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1000700: 14.166\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1000700: 7.105\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1000700: 12.177\n",
            "INFO:absl:Evaluating checkpoint step: 1000700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 607, 'targets': 75}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1000700: 0.000\n",
            "INFO:absl:eval/all_mix/bleu at step 1000700: 2.299\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 14.15, 95% confidence [13.70, 14.59]\n",
            "INFO:absl:rouge2 = 7.10, 95% confidence [6.79, 7.45]\n",
            "INFO:absl:rougeLsum = 12.18, 95% confidence [11.80, 12.57]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1000700: 14.146\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1000700: 7.098\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1000700: 12.176\n",
            "INFO:absl:Evaluating checkpoint step: 1005700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 607, 'targets': 75}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1005700: 31.283\n",
            "INFO:absl:eval/all_mix/bleu at step 1005700: 61.578\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 83.56, 95% confidence [82.95, 84.10]\n",
            "INFO:absl:rouge2 = 72.59, 95% confidence [71.69, 73.39]\n",
            "INFO:absl:rougeLsum = 80.69, 95% confidence [80.00, 81.33]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1005700: 83.561\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1005700: 72.593\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1005700: 80.690\n",
            "INFO:absl:Evaluating checkpoint step: 1005700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 607, 'targets': 75}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1005700: 31.283\n",
            "INFO:absl:eval/all_mix/bleu at step 1005700: 61.578\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 83.54, 95% confidence [82.91, 84.11]\n",
            "INFO:absl:rouge2 = 72.58, 95% confidence [71.72, 73.42]\n",
            "INFO:absl:rougeLsum = 80.69, 95% confidence [80.03, 81.34]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1005700: 83.545\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1005700: 72.583\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1005700: 80.693\n",
            "INFO:absl:Evaluating checkpoint step: 1005700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 607, 'targets': 75}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1005700: 31.283\n",
            "INFO:absl:eval/all_mix/bleu at step 1005700: 61.578\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 83.54, 95% confidence [82.97, 84.13]\n",
            "INFO:absl:rouge2 = 72.56, 95% confidence [71.75, 73.46]\n",
            "INFO:absl:rougeLsum = 80.69, 95% confidence [80.10, 81.36]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1005700: 83.538\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1005700: 72.563\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1005700: 80.694\n",
            "INFO:absl:Evaluating checkpoint step: 1005700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 607, 'targets': 75}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1005700: 31.283\n",
            "INFO:absl:eval/all_mix/bleu at step 1005700: 61.578\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 83.53, 95% confidence [82.90, 84.12]\n",
            "INFO:absl:rouge2 = 72.58, 95% confidence [71.71, 73.41]\n",
            "INFO:absl:rougeLsum = 80.67, 95% confidence [80.05, 81.32]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1005700: 83.533\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1005700: 72.582\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1005700: 80.674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a larger batch size for evaluation, which requires less memory.\n",
        "model.batch_size = train_batch_size \n",
        "model.eval(\n",
        "    mixture_or_task_name=\"all_mix\",\n",
        "    checkpoint_steps=\"all\",\n",
        "    split=\"test\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toHqyoVg-oSf",
        "outputId": "5dbdb822-c843-48e9-cd29-79d68488cfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://trecuofg/models/large_temperature_0/operative_config.gin\n",
            "ERROR:root:Path not found: gs://trecuofg/models/large_temperature_0/operative_config.gin\n",
            "INFO:absl:Adding task 'all_mix' with predict metric_fn(s).\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Skipping packing/padding for 'all_mix' since sequence length is None.\n",
            "INFO:absl:Setting sequence lengths to {'inputs': 589, 'targets': 120}\n",
            "INFO:absl:Evaluating checkpoint step: 1000700\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 589, 'targets': 120}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1000700: 0.000\n",
            "INFO:absl:eval/all_mix/bleu at step 1000700: 2.268\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 13.58, 95% confidence [13.24, 13.95]\n",
            "INFO:absl:rouge2 = 6.81, 95% confidence [6.55, 7.07]\n",
            "INFO:absl:rougeLsum = 11.85, 95% confidence [11.54, 12.16]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1000700: 13.585\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1000700: 6.813\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1000700: 11.849\n",
            "INFO:absl:Evaluating checkpoint step: 1000700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 589, 'targets': 120}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1000700: 0.000\n",
            "INFO:absl:eval/all_mix/bleu at step 1000700: 2.268\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 13.60, 95% confidence [13.24, 13.97]\n",
            "INFO:absl:rouge2 = 6.82, 95% confidence [6.56, 7.07]\n",
            "INFO:absl:rougeLsum = 11.86, 95% confidence [11.53, 12.16]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1000700: 13.595\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1000700: 6.819\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1000700: 11.856\n",
            "INFO:absl:Evaluating checkpoint step: 1000700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 589, 'targets': 120}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1000700: 0.000\n",
            "INFO:absl:eval/all_mix/bleu at step 1000700: 2.268\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 13.58, 95% confidence [13.23, 13.94]\n",
            "INFO:absl:rouge2 = 6.81, 95% confidence [6.56, 7.07]\n",
            "INFO:absl:rougeLsum = 11.86, 95% confidence [11.52, 12.17]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1000700: 13.578\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1000700: 6.808\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1000700: 11.859\n",
            "INFO:absl:Evaluating checkpoint step: 1000700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 589, 'targets': 120}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1000700: 0.000\n",
            "INFO:absl:eval/all_mix/bleu at step 1000700: 2.268\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 13.59, 95% confidence [13.24, 13.94]\n",
            "INFO:absl:rouge2 = 6.81, 95% confidence [6.54, 7.05]\n",
            "INFO:absl:rougeLsum = 11.85, 95% confidence [11.54, 12.16]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1000700: 13.592\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1000700: 6.809\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1000700: 11.849\n",
            "INFO:absl:Evaluating checkpoint step: 1005700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 589, 'targets': 120}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1005700: 28.200\n",
            "INFO:absl:eval/all_mix/bleu at step 1005700: 60.248\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 82.38, 95% confidence [81.91, 82.84]\n",
            "INFO:absl:rouge2 = 70.41, 95% confidence [69.69, 71.06]\n",
            "INFO:absl:rougeLsum = 79.62, 95% confidence [79.07, 80.12]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1005700: 82.378\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1005700: 70.410\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1005700: 79.624\n",
            "INFO:absl:Evaluating checkpoint step: 1005700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 589, 'targets': 120}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1005700: 28.200\n",
            "INFO:absl:eval/all_mix/bleu at step 1005700: 60.248\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 82.36, 95% confidence [81.88, 82.80]\n",
            "INFO:absl:rouge2 = 70.41, 95% confidence [69.71, 71.05]\n",
            "INFO:absl:rougeLsum = 79.58, 95% confidence [79.12, 80.12]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1005700: 82.356\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1005700: 70.413\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1005700: 79.578\n",
            "INFO:absl:Evaluating checkpoint step: 1005700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 589, 'targets': 120}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1005700: 28.200\n",
            "INFO:absl:eval/all_mix/bleu at step 1005700: 60.248\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 82.36, 95% confidence [81.90, 82.84]\n",
            "INFO:absl:rouge2 = 70.40, 95% confidence [69.77, 71.05]\n",
            "INFO:absl:rougeLsum = 79.60, 95% confidence [79.08, 80.14]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1005700: 82.357\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1005700: 70.400\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1005700: 79.603\n",
            "INFO:absl:Evaluating checkpoint step: 1005700\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/utils.py:780: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  num_parallel_calls=num_parallel_calls)\n",
            "INFO:absl:Padding 'all_mix' with sequence lengths: {'inputs': 589, 'targets': 120}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/all_mix/accuracy at step 1005700: 28.200\n",
            "INFO:absl:eval/all_mix/bleu at step 1005700: 60.248\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:rouge1 = 82.38, 95% confidence [81.89, 82.82]\n",
            "INFO:absl:rouge2 = 70.40, 95% confidence [69.75, 71.05]\n",
            "INFO:absl:rougeLsum = 79.60, 95% confidence [79.03, 80.14]\n",
            "INFO:absl:eval/all_mix/rouge1 at step 1005700: 82.381\n",
            "INFO:absl:eval/all_mix/rouge2 at step 1005700: 70.399\n",
            "INFO:absl:eval/all_mix/rougeLsum at step 1005700: 79.602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing it on the example questions provided\n",
        "question_1 = \"Did they have any clues? [CTX] Anna Politkovskaya [TURN] The murder remains unsolved, 2016\" #@param {type:\"string\"}\n",
        "question_2 = \"How did they target her email? [CTX] Anna Politkovskaya [TURN] The murder remains unsolved, 2016 [TURN] Did they have any clues? [TURN] probably FSB) are known to have targeted the webmail account of the murdered Russian journalist Anna Politkovskaya.\" #@param {type:\"string\"}\n",
        "question_3 = \"Did they have any murder suspects? [CTX] Anna Politkovskaya [TURN] The murder remains unsolved, 2016 [TURN] Did they have any clues? [TURN] probably FSB) are known to have targeted the webmail account of the murdered Russian journalist Anna Politkovskaya. [TURN] How did they target her email? [TURN] On 5 December 2005, RFIS initiated an attack against the account annapolitovskaya@US Provider1, by deploying malicious software [TURN] Did they get into trouble for that? [TURN] I don't know.\" #@param {type:\"string\"}\n",
        "question_4 = \"Is there anything else interesting in the article? [CTX] Anna Politkovskaya [TURN] The murder remains unsolved, 2016 [TURN] Did they have any clues? [TURN] probably FSB) are known to have targeted the webmail account of the murdered Russian journalist Anna Politkovskaya. [TURN] How did they target her email? [TURN] On 5 December 2005, RFIS initiated an attack against the account annapolitovskaya@US Provider1, by deploying malicious software [TURN] Did they get into trouble for that? [TURN] I don't know. [TURN] Did they have any murder suspects? [TURN] After the three Makhmudov brothers, Khadjikurbanov and Lom-Ali Gaitukayev were convicted in 2014, [TURN] Did they go to jail? [TURN] I don't know.\" #@param {type:\"string\"}\n",
        "\n",
        "questions = [question_1, question_2, question_3, question_4]\n",
        "\n",
        "now = time.time()\n",
        "# Write out the  questions to text files.\n",
        "predict_inputs_path = os.path.join(MODEL_DIR, \"predict_inputs_%d.txt\" % now)\n",
        "predict_outputs_path = os.path.join(MODEL_DIR, \"predict_outputs_%d.txt\" % now)\n",
        "# Manually apply preprocessing by prepending \"canard context:\".\n",
        "with tf.io.gfile.GFile(predict_inputs_path, \"w\") as f:\n",
        "  for q in questions:\n",
        "    f.write(\"canard context: %s\\n\" % q.lower())\n",
        "\n",
        "# Ignore any logging.\n",
        "with tf_verbosity_level('ERROR'):\n",
        "  model.batch_size = 4\n",
        "  model.predict(\n",
        "      input_file=predict_inputs_path,\n",
        "      output_file=predict_outputs_path,\n",
        "      # Select the most probable output token at each step.\n",
        "      temperature=0,\n",
        "  )\n",
        "\n",
        "# The output filename will have the checkpoint appended so we glob to get \n",
        "# the latest.\n",
        "prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])\n",
        "with tf.io.gfile.GFile(prediction_files[-1]) as f:\n",
        "  for q, a in zip(questions, f):\n",
        "    if q:\n",
        "      print(\"Context: \" + q)\n",
        "      print(\"rewrite: \" + a)\n",
        "      print()"
      ],
      "metadata": {
        "id": "5weptRW2FWWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting Model to the google bucket\n",
        "export_dir = os.path.join(MODEL_DIR, \"export\")\n",
        "\n",
        "model.batch_size = 1 # make one prediction per call\n",
        "saved_model_path = model.export(\n",
        "    export_dir,\n",
        "    checkpoint_step=-1,  # use most recent\n",
        "    beam_size=1,  # no beam search\n",
        "    temperature=0.0,  # sample according to predicted distribution\n",
        ")\n",
        "print(\"Model saved to:\", saved_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riRr2SgTHfoy",
        "outputId": "aa466948-ffa6-494f-cf5f-decfc2decb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://trecuofg/models/large_temperature_0/operative_config.gin\n",
            "ERROR:root:Path not found: gs://trecuofg/models/large_temperature_0/operative_config.gin\n",
            "eval_on_tpu ignored because use_tpu is False.\n",
            "/usr/local/lib/python3.7/dist-packages/mesh_tensorflow/transformer/dataset.py:380: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
            "==================================\n",
            "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
            "<tf.Operation 'assert_less_equal/Assert/Assert' type=Assert>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
            "    return target(*args, **kwargs)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/check_ops.py\", line 947, in assert_less_equal\n",
            "    np.less_equal, x, y, data, summarize, message, name)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/check_ops.py\", line 373, in _binary_assert\n",
            "    return control_flow_ops.Assert(condition, data, summarize=summarize)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
            "    return target(*args, **kwargs)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
            "    error_in_function=error_in_function)\n",
            "==================================\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: b'gs://trecuofg/models/large_temperature_0/export/1660202739'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install tensorflow-text\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fKauI3ySdsO",
        "outputId": "c4b05190-8671-4af3-a8fc-193cd2d7b046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Collecting tensorflow<2.10,>=2.9.0\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 5.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.47.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (14.0.6)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (4.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.1.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 54.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 75.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.15.0)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 33.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (21.3)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.26.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-text-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the Exported model\n",
        "import tensorflow as tf\n",
        "import tensorflow_text  \n",
        "\n",
        "# TODO change this to load a different model\n",
        "saved_model_path = \"gs://trecuofg/models/large_temperature_0/export/1660202739\"\n",
        "\n",
        "def load_predict_fn(model_path):\n",
        "  if tf.executing_eagerly():\n",
        "    print(\"Loading SavedModel in eager mode.\")\n",
        "    imported = tf.saved_model.load(model_path, [\"serve\"])\n",
        "    return lambda x: imported.signatures['serving_default'](tf.constant(x))['outputs'].numpy()\n",
        "  else:\n",
        "    print(\"Loading SavedModel in tf 1.x graph mode.\")\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    sess = tf.compat.v1.Session()\n",
        "    meta_graph_def = tf.compat.v1.saved_model.load(sess, [\"serve\"], model_path)\n",
        "    signature_def = meta_graph_def.signature_def[\"serving_default\"]\n",
        "    return lambda x: sess.run(\n",
        "        fetches=signature_def.outputs[\"outputs\"].name, \n",
        "        feed_dict={signature_def.inputs[\"input\"].name: x}\n",
        "    )\n",
        "\n",
        "predict_fn = load_predict_fn(saved_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3kFViSyiCFV",
        "outputId": "490b5e18-3d1d-4517-8749-9ec74330d141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SavedModel in eager mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(1024,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(1024,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(1024,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(1024,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(1024, 1024) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for predicting the queries\n",
        "def answer(question):\n",
        "  return predict_fn([question])[0].decode('utf-8')"
      ],
      "metadata": {
        "id": "tDl4wCnXifVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Executing the example queries\n",
        "targets =  [\n",
        "           \"Aside from the discontinued case against a convicted killer, how is the murder of Anna Politkovskaya similar to any other cases?\",\n",
        "           \"Why did Superstar Billy Graham return to the WWWF?\",\n",
        "           \"What was Superstar Billy Graham's agreement with McMahon?\",\n",
        "           \"How did people respond to Superstar Billy Graham's return?\"\n",
        "]\n",
        "\n",
        "questions = [\"canard context: Is it similar to any other cases? [CTX] Anna Politkovskaya [TURN] The murder remains unsolved, 2016 [TURN] Did they have any clues? [TURN] probably FSB) are known to have targeted the webmail account of the murdered Russian journalist Anna Politkovskaya. [TURN] How did they target her email? [TURN] On 5 December 2005, RFIS initiated an attack against the account annapolitovskaya@US Provider1, by deploying malicious software [TURN] Did they get into trouble for that? [TURN] I don't know. [TURN] Did they have any murder suspects? [TURN] After the three Makhmudov brothers, Khadjikurbanov and Lom-Ali Gaitukayev were convicted in 2014, [TURN] Did they go to jail? [TURN] I don't know. [TURN] Is there anything else interesting in the article? [TURN] In accordance with Russian law there is a 15-year statute of limitation for the 'particularly grave' crime of first degree murder. [TURN] Are they close to solving it? [TURN] In May that year the case against him was discontinued because the statute of limitations had expired.\",\n",
        "             \"canard context: Why did he return to the WWWF? [CTX] Superstar Billy Graham [TURN] Return to WWWF (1977-1981)\",\n",
        "             \"canard context: What was his agreement with McMahon? [CTX] Superstar Billy Graham [TURN] Return to WWWF (1977-1981) [TURN] Why did he return to the WWWF? [TURN] an agreement with promoter Vincent J. McMahon (Senior\",\n",
        "             \"canard context: How did people respond to his return? [CTX] Superstar Billy Graham [TURN] Return to WWWF (1977-1981) [TURN] Why did he return to the WWWF? [TURN] an agreement with promoter Vincent J. McMahon (Senior [TURN] What was his agreement with McMahon? [TURN] I don't know.\"\n",
        "            ]\n",
        "\n",
        "def test_examples():\n",
        "  for i, question in enumerate(questions):\n",
        "      print(\"Target: \" + targets[i])\n",
        "      print(\"Rewrite: \" + answer(question))\n",
        "      print()"
      ],
      "metadata": {
        "id": "u_8o4k0ckCZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(1):\n",
        "  test_examples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYJ9cxgtkEzQ",
        "outputId": "c5f84c17-9a95-4323-c334-8e3bc4e991a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: Aside from the discontinued case against a convicted killer, how is the murder of Anna Politkovskaya similar to any other cases?\n",
            "Rewrite: Is the murder of Anna Politkovskaya similar to any other cases?\n",
            "\n",
            "Target: Why did Superstar Billy Graham return to the WWWF?\n",
            "Rewrite: Why did Billy Graham return to the WWWF?\n",
            "\n",
            "Target: What was Superstar Billy Graham's agreement with McMahon?\n",
            "Rewrite: What was Billy Graham's agreement with McMahon?\n",
            "\n",
            "Target: How did people respond to Superstar Billy Graham's return?\n",
            "Rewrite: How did people respond to Billy Graham's return to the WWWF?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kV-YnGyVSx06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhijfMm2kjjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hjUgfSzzTR_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}